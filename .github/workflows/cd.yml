name: Documents Microservice CD

on:
  push:
    branches: [main]

permissions:
  contents: read

env:
  TF_BACKEND_BUCKET: ${{ secrets.TF_BACKEND_BUCKET }}
  TF_BACKEND_KEY: infra/terraform/aws/terraform.tfstate
  TF_BACKEND_REGION: ${{ secrets.AWS_REGION }}
  TF_BACKEND_DDB_TABLE: ${{ secrets.TF_BACKEND_DDB_TABLE }}

jobs:
  build-and-push:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@e468171a9de216ec08956ac3ada2f0791b6bd435

      - name: Log in to Docker Hub
        uses: docker/login-action@9780b0c442fbb1117ed29e0efdff1e18412f7567
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Extract metadata (tags, labels) for Docker
        id: meta
        uses: docker/metadata-action@8e5442c4ef9f78752691e2d8f8d19755c6f78e81
        with:
          images: ${{ secrets.DOCKERHUB_USERNAME }}/documents-microservice
          tags: |
            type=sha,prefix=main-
            type=raw,value=latest

      - name: Build and push Docker image
        uses: docker/build-push-action@48aba3b46d1b1fec4febb7c5d0c644b249a11355
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64

      - name: Image digest
        run: echo ${{ steps.meta.outputs.digest }}

  sonar-tracking:
    name: SonarCloud Tracking (Main Branch)
    runs-on: ubuntu-latest
    needs: build-and-push
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: "1.25"
          cache: true

      - name: Install dependencies
        run: |
          go mod download
          go mod tidy

      - name: Install golangci-lint
        run: |
          curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b $(go env GOPATH)/bin
          echo "$(go env GOPATH)/bin" >> $GITHUB_PATH

      - name: Run golangci-lint
        run: |
          golangci-lint run --output.checkstyle.path=golangci-lint-report.xml || true

      - name: Run Tests with Coverage
        run: |
          echo "Running unit tests..."
          go test ./internal/.../tests/... -v
          
          echo "Generating coverage report..."
          go test -coverpkg=./internal/... ./internal/.../tests -coverprofile=coverage.out -covermode=atomic
          
          echo "Coverage Summary:"
          go tool cover -func=coverage.out | tail -1

      - name: SonarCloud Scan (Main Branch)
        uses: SonarSource/sonarcloud-github-action@ffc3010689be73b8e5ae0c57ce35968afd7909e8 # v5.0.0
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        with:
          args: >
            -Dsonar.branch.name=main

  infra-apply:
    name: Provision Microservice AWS Resources (Terraform) - Phase 1
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_wrapper: false

      - name: Terraform Init (infra - remote state)
        working-directory: infra/terraform/aws
        run: |
          terraform init -input=false -upgrade \
            -backend-config="bucket=$TF_BACKEND_BUCKET" \
            -backend-config="key=$TF_BACKEND_KEY" \
            -backend-config="region=$TF_BACKEND_REGION" \
            -backend-config="dynamodb_table=$TF_BACKEND_DDB_TABLE" \
            -backend-config="encrypt=true"

      - name: Terraform Validate (infra)
        working-directory: infra/terraform/aws
        run: terraform validate

      - name: Clean up scheduled-for-deletion secrets
        run: |
          # List all secrets with "documents" in the name
          echo "Checking for secrets scheduled for deletion..."
          aws secretsmanager list-secrets --region ${{ secrets.AWS_REGION }} \
            --filters Key=name,Values=documents --output json | \
            jq -r '.SecretList[] | select(.DeletedDate != null) | .ARN' | \
            while read -r secret_arn; do
              if [ -n "$secret_arn" ]; then
                echo "Force deleting secret: $secret_arn"
                aws secretsmanager delete-secret --secret-id "$secret_arn" \
                  --region ${{ secrets.AWS_REGION }} \
                  --force-delete-without-recovery || true
              fi
            done
          # Wait longer for AWS to process the deletion
          echo "Waiting 30 seconds for AWS to process..."
          sleep 30

      - name: Terraform Apply (microservice resources - without ALB)
        working-directory: infra/terraform/aws
        run: |
          # Apply only resources that don't depend on ALB
          # ALB will be created later when ingress is deployed in app-deploy step
          terraform apply -auto-approve -input=false \
            -var "tf_backend_bucket=$TF_BACKEND_BUCKET" \
            -target random_id.suffix \
            -target aws_s3_bucket.documents \
            -target aws_s3_bucket_public_access_block.this \
            -target aws_dynamodb_table.documents \
            -target aws_secretsmanager_secret.app \
            -target data.aws_iam_policy_document.documents_policy \
            -target aws_iam_policy.documents \
            -target module.irsa

  k8s-apply:
    name: Deploy Monitoring Config (Dashboards & Alerts) - Phase 2
    runs-on: ubuntu-latest
    needs: [infra-apply]
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_wrapper: false

      - name: Terraform Init (infra dir to read outputs)
        working-directory: infra/terraform/aws
        run: |
          terraform init -input=false \
            -backend-config="bucket=$TF_BACKEND_BUCKET" \
            -backend-config="key=$TF_BACKEND_KEY" \
            -backend-config="region=$TF_BACKEND_REGION" \
            -backend-config="dynamodb_table=$TF_BACKEND_DDB_TABLE" \
            -backend-config="encrypt=true"

      - name: Read infra outputs (EKS cluster info from shared infra)
        id: infra_outputs
        run: |
          echo "CLUSTER_NAME=$(terraform -chdir=infra/terraform/aws output -raw cluster_name)" >> $GITHUB_OUTPUT
          echo "CLUSTER_ENDPOINT=$(terraform -chdir=infra/terraform/aws output -raw cluster_endpoint)" >> $GITHUB_OUTPUT
          echo "CLUSTER_CA=$(terraform -chdir=infra/terraform/aws output -raw cluster_ca_certificate)" >> $GITHUB_OUTPUT

      - name: Terraform Init (k8s - remote state)
        working-directory: k8s/terraform/aws
        run: |
          terraform init -input=false -upgrade \
            -backend-config="bucket=$TF_BACKEND_BUCKET" \
            -backend-config="key=k8s/terraform/aws/terraform.tfstate" \
            -backend-config="region=$TF_BACKEND_REGION" \
            -backend-config="dynamodb_table=$TF_BACKEND_DDB_TABLE" \
            -backend-config="encrypt=true"

      - name: Import existing K8s/Helm resources into state (idempotency)
        working-directory: k8s/terraform/aws
        env:
          TF_VAR_aws_region: ${{ secrets.AWS_REGION }}
          TF_VAR_cluster_name: ${{ steps.infra_outputs.outputs.CLUSTER_NAME }}
          TF_VAR_cluster_endpoint: ${{ steps.infra_outputs.outputs.CLUSTER_ENDPOINT }}
          TF_VAR_cluster_ca_certificate: ${{ steps.infra_outputs.outputs.CLUSTER_CA }}
        run: |
          set -euo pipefail
          # Importar ConfigMap del dashboard si ya existe
          terraform import -input=false -no-color kubernetes_config_map.grafana_dashboard monitoring/documents-service-dashboard || true
          # Importar PrometheusRule si ya existe
          terraform import -input=false -no-color kubernetes_manifest.prometheus_rule '{"apiVersion":"monitoring.coreos.com/v1","kind":"PrometheusRule","metadata":{"name":"documents-service-alerts","namespace":"monitoring"}}' || true

      - name: Terraform Validate (k8s)
        working-directory: k8s/terraform/aws
        run: terraform validate

      - name: Terraform Apply (monitoring stack)
        working-directory: k8s/terraform/aws
        env:
          TF_LOG: INFO
        run: |
          terraform apply -auto-approve -input=false \
            -var "aws_region=${{ secrets.AWS_REGION }}" \
            -var "cluster_name=${{ steps.infra_outputs.outputs.CLUSTER_NAME }}" \
            -var "cluster_endpoint=${{ steps.infra_outputs.outputs.CLUSTER_ENDPOINT }}" \
            -var "cluster_ca_certificate=${{ steps.infra_outputs.outputs.CLUSTER_CA }}"

  app-deploy:
    name: Deploy Application to EKS (kubectl/kustomize) - Phase 3
    runs-on: ubuntu-latest
    needs: [build-and-push, infra-apply, k8s-apply]
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Install kubectl
        run: |
          KUBECTL_VERSION=$(curl -sL https://dl.k8s.io/release/stable.txt)
          curl -LO "https://dl.k8s.io/release/${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/kubectl
          kubectl version --client

      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Setup Terraform (to read outputs from remote state)
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_wrapper: false

      - name: Terraform Init (infra dir to read outputs)
        working-directory: infra/terraform/aws
        run: |
          terraform init -input=false \
            -backend-config="bucket=$TF_BACKEND_BUCKET" \
            -backend-config="key=$TF_BACKEND_KEY" \
            -backend-config="region=$TF_BACKEND_REGION" \
            -backend-config="dynamodb_table=$TF_BACKEND_DDB_TABLE" \
            -backend-config="encrypt=true"

      - name: Update kubeconfig (connect kubectl to EKS)
        run: |
          CLUSTER_NAME=$(terraform -chdir=infra/terraform/aws output -raw cluster_name)
          aws eks update-kubeconfig --name "$CLUSTER_NAME" --region ${{ secrets.AWS_REGION }}

      - name: Prepare AWS Secrets for app
        env:
          TF_DIR: infra/terraform/aws
        run: |
          SECRET_NAME=$(terraform -chdir=$TF_DIR output -raw secretsmanager_secret_name)
          RABBIT_URL=$(terraform -chdir=$TF_DIR output -raw rabbitmq_amqp_url)
          S3_BUCKET=$(terraform -chdir=$TF_DIR output -raw s3_bucket)
          PROCESSED_MSGS_TABLE=$(terraform -chdir=$TF_DIR output -raw rabbitmq_processed_messages_table_name)
          aws secretsmanager put-secret-value \
            --secret-id "$SECRET_NAME" \
            --secret-string "$(jq -n \
              --arg port "${{ secrets.APP_PORT }}" \
              --arg table "$(terraform -chdir=$TF_DIR output -raw dynamodb_table)" \
              --arg processed_table "$PROCESSED_MSGS_TABLE" \
              --arg region "${{ secrets.AWS_REGION }}" \
              --arg bucket "$S3_BUCKET" \
              --arg rabbit "$RABBIT_URL" \
              --arg consumer_queue "${{ secrets.RABBITMQ_CONSUMER_QUEUE }}" \
              --arg auth_request_queue "${{ secrets.RABBITMQ_AUTH_REQUEST_QUEUE }}" \
              --arg auth_result_queue "${{ secrets.RABBITMQ_AUTH_RESULT_QUEUE }}" \
              --arg jwt_secret "${{ secrets.JWT_SECRET }}" \
              --arg aws_access_key "${{ secrets.AWS_ACCESS_KEY_ID }}" \
              --arg aws_secret_key "${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
              '{
                APP_PORT: $port,
                DYNAMODB_TABLE: $table,
                DYNAMODB_PROCESSED_MESSAGES_TABLE: $processed_table,
                DYNAMODB_ENDPOINT: "",
                AWS_ACCESS_KEY_ID: $aws_access_key,
                AWS_SECRET_ACCESS_KEY: $aws_secret_key,
                AWS_REGION: $region,
                S3_BUCKET: $bucket,
                S3_ENDPOINT: "",
                S3_USE_PATH_STYLE: "false",
                S3_PUBLIC_BASE_URL: "",
                RABBITMQ_URL: $rabbit,
                RABBITMQ_CONSUMER_QUEUE: $consumer_queue,
                RABBITMQ_AUTH_REQUEST_QUEUE: $auth_request_queue,
                RABBITMQ_AUTH_RESULT_QUEUE: $auth_result_queue,
                JWT_SECRET: $jwt_secret
              }')" || true

      - name: Render K8s templates with infra outputs
        env:
          TF_DIR: infra/terraform/aws
        run: |
          set -euo pipefail
          SECRET_NAME=$(terraform -chdir=$TF_DIR output -raw secretsmanager_secret_name)
          REGION='${{ secrets.AWS_REGION }}'
          IRSA_ROLE=$(terraform -chdir=$TF_DIR output -raw irsa_role_arn)
          # Sustituir placeholders en External Secrets
          sed -i "s|__AWS_REGION__|$REGION|g" k8s/overlays/prod/externalsecrets.yaml
          sed -i "s|__AWS_SECRET_NAME__|$SECRET_NAME|g" k8s/overlays/prod/externalsecrets.yaml
          # Sustituir placeholder de IRSA role en ServiceAccount
          sed -i "s|IRSA_ROLE_ARN|$IRSA_ROLE|g" k8s/base/service-account.yaml
          
          # Debug: Ver los valores sustituidos
          echo "=== Values used ==="
          echo "SECRET_NAME: $SECRET_NAME"
          echo "REGION: $REGION"
          echo "IRSA_ROLE: $IRSA_ROLE"
          echo "=== Checking rendered External Secret ==="
          grep -A 2 "__AWS_" k8s/overlays/prod/externalsecrets.yaml || echo "All placeholders replaced successfully"

      - name: Verify External Secrets Operator is installed
        run: |
          echo "=== Checking External Secrets Operator ==="
          kubectl get pods -n external-secrets || echo "External Secrets namespace not found"
          echo ""
          echo "=== Checking External Secrets Operator logs ==="
          kubectl logs -n external-secrets -l app.kubernetes.io/name=external-secrets --tail=50 || echo "Cannot get logs"
          
      - name: Delete old ExternalSecret if exists
        run: |
          echo "=== Deleting old ExternalSecret ==="
          kubectl -n documents delete externalsecret documents-secrets --ignore-not-found=true
          sleep 5
          echo "Old ExternalSecret deleted"
          
      - name: Deploy application
        run: |
          kubectl apply -k k8s/overlays/prod/
          
      - name: Force rolling update (to pick up new image)
        run: |
          echo "Triggering rolling update to ensure new image is used..."
          kubectl rollout restart deployment/documents-service -n documents
          
      - name: Check External Secret status
        run: |
          echo "=== Checking ExternalSecret status ==="
          kubectl -n documents get externalsecret documents-secrets -o yaml || echo "ExternalSecret not found"
          echo ""
          echo "=== Checking ExternalSecret events ==="
          kubectl -n documents describe externalsecret documents-secrets || true
          
      - name: Wait for External Secrets to create secret
        run: |
          echo "Waiting for ExternalSecret to create documents-secrets..."
          for i in {1..12}; do
            if kubectl -n documents get secret documents-secrets &>/dev/null; then
              echo "Secret created successfully!"
              kubectl -n documents get secret documents-secrets
              break
            fi
            echo "Waiting... (attempt $i/12)"
            sleep 5
          done
          
          # If still not created, show ExternalSecret status
          if ! kubectl -n documents get secret documents-secrets &>/dev/null; then
            echo "ERROR: Secret still not created after 60 seconds"
            echo "=== ExternalSecret status ==="
            kubectl -n documents get externalsecret documents-secrets -o yaml
            exit 1
          fi
          
      - name: Check deployment status and debug
        run: |
          echo "=== Checking deployment status ==="
          kubectl -n documents get deploy documents-service -o yaml
          echo ""
          echo "=== Checking pod status ==="
          kubectl -n documents get pods -l app=documents-service
          echo ""
          echo "=== Checking pod logs (if any pods exist) ==="
          kubectl -n documents logs -l app=documents-service --tail=50 || echo "No logs available yet"
          echo ""
          echo "=== Describing deployment ==="
          kubectl -n documents describe deploy documents-service || true
          
      - name: Wait for deployment rollout
        run: |
          kubectl -n documents rollout status deploy/documents-service --timeout=180s

      - name: Wait for ALB to be provisioned
        run: |
          echo "Waiting for ALB to get hostname assigned..."
          for i in {1..30}; do
            ALB_HOSTNAME=$(kubectl -n documents get ingress documents-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            if [ -n "$ALB_HOSTNAME" ]; then
              echo "✓ ALB provisioned: $ALB_HOSTNAME"
              break
            fi
            echo "Waiting for ALB... (attempt $i/30)"
            sleep 10
          done
          
          # Mostrar estado actual del ingress
          if [ -z "$ALB_HOSTNAME" ]; then
            echo "ALB still pending after 5 minutes"
            echo "=== Current Ingress Status ==="
            kubectl -n documents get ingress documents-ingress
            echo ""
            echo "=== Ingress Events ==="
            kubectl -n documents describe ingress documents-ingress | grep -A 20 "Events:" || true
          else
            # ALB exists, now apply API Gateway integration
            echo ""
            echo "=== ALB Found: $ALB_HOSTNAME ==="
            echo "Applying API Gateway integration..."
            
            cd infra/terraform/aws
            terraform init -input=false
            
            # Apply ONLY ALB-dependent resources
            terraform apply -auto-approve -input=false \
              -target data.aws_lb.documents_alb \
              -target data.aws_lb_listener.documents_alb_http \
              -target aws_security_group_rule.vpc_link_to_alb \
              -target aws_apigatewayv2_integration.documents \
              -target aws_apigatewayv2_route.documents_api \
              -var "tf_backend_bucket=$TF_BACKEND_BUCKET"
            
            cd ../../..
          fi

      - name: Display Access Information
        run: |
          echo "=== Deployment Summary ==="
          echo ""
          echo "API Gateway (Shared Infrastructure):"
          API_GW_URL=$(terraform -chdir=infra/terraform/aws output -raw api_gateway_url 2>/dev/null || echo "")
          if [ -n "$API_GW_URL" ]; then
            echo "  API Gateway Endpoint: $API_GW_URL"
            HEALTH_URL=$(terraform -chdir=infra/terraform/aws output -raw api_gateway_health_check_url 2>/dev/null || echo "")
            echo "  Health Check: $HEALTH_URL"
            echo ""
            echo " Note: API Gateway routes to microservice ALB via VPC Link"
          else
            echo "  Pending... Waiting for API Gateway integration to complete"
          fi
          echo ""
          echo ""
          echo "Application Load Balancer (Direct Access):"
          ALB_HOSTNAME=$(kubectl -n documents get ingress documents-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ -n "$ALB_HOSTNAME" ]; then
            echo "  Hostname: $ALB_HOSTNAME"
            echo "  API: http://$ALB_HOSTNAME/api/docs/documents"
            echo "  Health: http://$ALB_HOSTNAME/healthz"
            echo "  Swagger: http://$ALB_HOSTNAME/api/docs/swagger"
          else
            echo "  Pending... Waiting for ALB to be provisioned"
            echo "  This can take 2-5 minutes"
            kubectl -n documents get ingress documents-ingress -o yaml
          fi
          echo ""
          echo ""
          echo "Grafana ALB (Shared Infrastructure):"
          GRAFANA_ALB=$(kubectl -n monitoring get ingress grafana-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ -n "$GRAFANA_ALB" ]; then
            echo "  Hostname: $GRAFANA_ALB"
            echo "  Access: http://$GRAFANA_ALB"
            echo "  (user: admin, pass: admin)"
            echo "  Note: Deployed by shared infrastructure"
          else
            echo "  Not found (Grafana deployed by shared infrastructure)"
          fi
          echo ""
          echo "Prometheus (internal, shared infrastructure):"
          echo "  kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090"
          echo "  Note: Prometheus is deployed by shared infrastructure"
          echo ""
