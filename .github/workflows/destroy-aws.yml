name: Destroy AWS (EKS + Infra)

on:
  workflow_dispatch:
    inputs:
      confirm:
        description: 'Escribe EXACTAMENTE: I UNDERSTAND (para confirmar el destroy)'
        required: true
        default: ''
      delete_backend:
        description: 'Eliminar también el S3 backend y la tabla DynamoDB de locks? (true/false)'
        required: true
        default: 'false'

permissions:
  contents: read

env:
  TF_BACKEND_BUCKET: ${{ secrets.TF_BACKEND_BUCKET }}
  TF_BACKEND_KEY: infra/terraform/aws/terraform.tfstate
  TF_BACKEND_REGION: ${{ secrets.AWS_REGION }}
  TF_BACKEND_DDB_TABLE: ${{ secrets.TF_BACKEND_DDB_TABLE }}

jobs:
  destroy:
    name: Destroy Microservice Resources (NOT Shared Infrastructure)
    runs-on: ubuntu-latest

    steps:
      - name: Guard rail confirmation
        run: |
          if [ "${{ github.event.inputs.confirm }}" != "I UNDERSTAND" ]; then
            echo "Falta confirmación exacta: I UNDERSTAND"
            exit 1
          fi
          echo "Confirmación OK - Destruyendo SOLO recursos del microservicio"
          echo "La infraestructura compartida (VPC, EKS, RabbitMQ) NO será destruida"

      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region:            ${{ secrets.AWS_REGION }}

      - name: Install kubectl
        run: |
          set -e
          KUBECTL_VERSION=$(curl -sL https://dl.k8s.io/release/stable.txt)
          curl -LO "https://dl.k8s.io/release/${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/kubectl

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_wrapper: false
      
      - name: Install jq (for JSON parsing)
        run: sudo apt-get update && sudo apt-get install -y jq

      # ---------------------------
      # 1) Conectarnos al cluster (que existe en infraestructura compartida)
      # ---------------------------
      - name: Init Terraform (infra dir) para leer outputs
        working-directory: infra/terraform/aws
        run: |
          terraform init -input=false \
            -backend-config="bucket=$TF_BACKEND_BUCKET" \
            -backend-config="key=$TF_BACKEND_KEY" \
            -backend-config="region=$TF_BACKEND_REGION" \
            -backend-config="dynamodb_table=$TF_BACKEND_DDB_TABLE" \
            -backend-config="encrypt=true"

      - name: Update kubeconfig (EKS desde shared infrastructure)
        id: kubeconfig
        continue-on-error: true
        run: |
          set -e
          CLUSTER_NAME=$(terraform -chdir=infra/terraform/aws output -raw cluster_name)
          echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT
          aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "${{ secrets.AWS_REGION }}"
          kubectl version --client
          kubectl get nodes || true

      # ---------------------------
      # 2) Limpiar recursos K8s del microservicio
      # ---------------------------
      - name: Delete microservice-specific K8s resources
        if: steps.kubeconfig.outcome == 'success'
        continue-on-error: true
        run: |
          set -e
          echo "Eliminando recursos K8s del microservicio documents"
          
          # Guardar nombres de LoadBalancers para eliminarlos después
          DOCUMENTS_LB=$(kubectl -n documents get svc documents-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          GRAFANA_LB=$(kubectl -n monitoring get svc kube-prometheus-stack-grafana -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          
          if [ -n "$DOCUMENTS_LB" ]; then
            echo "documents_lb_hostname=$DOCUMENTS_LB" >> $GITHUB_OUTPUT
          fi
          if [ -n "$GRAFANA_LB" ]; then
            echo "grafana_lb_hostname=$GRAFANA_LB" >> $GITHUB_OUTPUT
          fi
          
          # Esperar a que los pods terminen antes de eliminar recursos
          echo "Esperando a que deployments terminen..."
          kubectl -n documents rollout status deployment/documents-service --timeout=60s || true
          
          # Eliminar aplicación
          echo "Eliminando aplicación con kustomize..."
          kubectl delete -k k8s/overlays/prod/ --ignore-not-found --wait=true --timeout=300s || true
          
          # Eliminar namespace completo si existe (para limpiar totalmente)
          echo "Eliminando namespace documents..."
          kubectl delete namespace documents --ignore-not-found --wait=true --timeout=180s || true
          
          # ConfigMap del dashboard específico
          kubectl delete configmap documents-service-dashboard -n monitoring --ignore-not-found || true
          # PrometheusRule específico
          kubectl delete prometheusrule documents-service-alerts -n monitoring --ignore-not-found || true
          # ServiceMonitor específico
          kubectl delete servicemonitor documents-service -n monitoring --ignore-not-found || true

      - name: Wait for LoadBalancers and PVCs to be deleted
        if: steps.kubeconfig.outcome == 'success'
        continue-on-error: true
        run: |
          echo "Esperando a que LoadBalancers y PVCs se eliminen..."
          # Esperar hasta 5 minutos para que AWS elimine los LoadBalancers
          for i in {1..10}; do
            LB_COUNT=$(kubectl get svc -A -o json | jq '.items[] | select(.metadata.name | contains("documents") or contains("grafana")) | .status.loadBalancer.ingress' | jq -s 'length' || echo "0")
            if [ "$LB_COUNT" = "0" ]; then
              echo "Todos los LoadBalancers han sido eliminados"
              break
            fi
            echo "Esperando... ($i/10) LoadBalancers restantes: $LB_COUNT"
            sleep 30
          done
          
          # Verificar PVCs pendientes de eliminación
          PVC_COUNT=$(kubectl get pvc -n documents 2>/dev/null | wc -l || echo "0")
          if [ "$PVC_COUNT" -gt "1" ]; then
            echo "Forzando eliminación de PVCs en namespace documents..."
            kubectl delete pvc --all -n documents --force --grace-period=0 || true
          fi

      - name: Terraform Init (k8s)
        working-directory: k8s/terraform/aws
        run: |
          terraform init -input=false -upgrade \
            -backend-config="bucket=$TF_BACKEND_BUCKET" \
            -backend-config="key=k8s/terraform/aws/terraform.tfstate" \
            -backend-config="region=$TF_BACKEND_REGION" \
            -backend-config="dynamodb_table=$TF_BACKEND_DDB_TABLE" \
            -backend-config="encrypt=true"

      - name: Read infra outputs (para variables del provider k8s/helm)
        id: infra_outputs_destroy
        continue-on-error: true
        run: |
          echo "CLUSTER_NAME=$(terraform -chdir=infra/terraform/aws output -raw cluster_name)" >> $GITHUB_OUTPUT
          echo "CLUSTER_ENDPOINT=$(terraform -chdir=infra/terraform/aws output -raw cluster_endpoint)" >> $GITHUB_OUTPUT
          echo "CLUSTER_CA=$(terraform -chdir=infra/terraform/aws output -raw cluster_ca_certificate)" >> $GITHUB_OUTPUT

      - name: Terraform Destroy (k8s monitoring stack)
        if: steps.infra_outputs_destroy.outcome == 'success'
        working-directory: k8s/terraform/aws
        env:
          TF_LOG: INFO
        continue-on-error: true
        run: |
          set -e
          echo "Destruyendo monitoring stack (Prometheus/Grafana)..."
          
          # Primero intentar eliminar Helm release manualmente si existe
          if kubectl get namespace monitoring >/dev/null 2>&1; then
            echo "Eliminando Helm release kube-prometheus-stack..."
            kubectl delete namespace monitoring --wait=true --timeout=300s || true
            sleep 30
          fi
          
          # Ahora hacer terraform destroy
          terraform destroy -auto-approve -input=false \
            -var "aws_region=${{ secrets.AWS_REGION }}" \
            -var "cluster_name=${{ steps.infra_outputs_destroy.outputs.CLUSTER_NAME }}" \
            -var "cluster_endpoint=${{ steps.infra_outputs_destroy.outputs.CLUSTER_ENDPOINT }}" \
            -var "cluster_ca_certificate=${{ steps.infra_outputs_destroy.outputs.CLUSTER_CA }}"
          
          echo "Terraform destroy completado para monitoring stack"

      # ---------------------------
      # 3) Destroy microservice-specific AWS resources (S3, DynamoDB, IAM)
      # ---------------------------
      - name: Get microservice DynamoDB table name
        working-directory: infra/terraform/aws
        id: get_microservice_table
        continue-on-error: true
        run: |
          # Intentar obtener el nombre de la tabla de DynamoDB del microservicio
          TABLE_NAME=$(terraform output -raw dynamodb_table 2>/dev/null || echo "")
          if [ -n "$TABLE_NAME" ]; then
            echo "table_name=$TABLE_NAME" >> $GITHUB_OUTPUT
            echo "TABLE_NAME=$TABLE_NAME" >> $GITHUB_ENV
          else
            echo "No DynamoDB table found for microservice (or already destroyed)"
          fi

      - name: Disable DynamoDB PITR before destroy
        if: env.TABLE_NAME != ''
        continue-on-error: true
        run: |
          echo "Disabling Point-in-time Recovery for DynamoDB table: $TABLE_NAME"
          aws dynamodb update-continuous-backups \
            --table-name "$TABLE_NAME" \
            --point-in-time-recovery-specification PointInTimeRecoveryEnabled=false \
            --region ${{ secrets.AWS_REGION }} || echo "Could not disable PITR (table might not exist or already disabled)"
          
          # Wait a moment for the change to propagate
          sleep 2

      - name: Terraform Destroy (microservice resources)
        working-directory: infra/terraform/aws
        env:
          TF_LOG: INFO
        run: |
          set -e
          echo "Destruyendo recursos AWS del microservicio..."
          
          # Primero intentar con terraform destroy normal
          echo "Intento 1: Terraform destroy normal"
          terraform destroy -auto-approve -input=false \
            -var "tf_backend_bucket=$TF_BACKEND_BUCKET" || DESTROY_FAILED=true
          
          if [ "${DESTROY_FAILED}" = "true" ]; then
            echo "Terraform destroy falló, intentando con -refresh=false"
            
            # Si falla, intentar sin refresh para evitar problemas con el state
            terraform destroy -auto-approve -input=false -refresh=false \
              -var "tf_backend_bucket=$TF_BACKEND_BUCKET" || echo "Segundo intento también falló"
          fi
          
          echo "Terraform destroy completado para recursos del microservicio"

      - name: Force destroy resources if Terraform failed
        if: failure()
        continue-on-error: true
        working-directory: infra/terraform/aws
        run: |
          set -e
          echo "Intentando eliminar recursos manualmente debido a errores de Terraform..."
          
          # Intentar leer los outputs del state para destruir manualmente
          S3_BUCKET=$(terraform output -raw s3_bucket 2>/dev/null || echo "")
          DYNAMODB_TABLE=$(terraform output -raw dynamodb_table 2>/dev/null || echo "")
          SECRET_ARN=$(terraform output -raw secretsmanager_secret_arn 2>/dev/null || echo "")
          IRSA_ROLE_ARN=$(terraform output -raw irsa_role_arn 2>/dev/null || echo "")
          
          echo "Recursos encontrados:"
          echo "  S3 Bucket: $S3_BUCKET"
          echo "  DynamoDB Table: $DYNAMODB_TABLE"
          echo "  Secret ARN: $SECRET_ARN"
          echo "  IRSA Role ARN: $IRSA_ROLE_ARN"
          
          # Eliminar S3 bucket
          if [ -n "$S3_BUCKET" ]; then
            echo "Eliminando S3 bucket: $S3_BUCKET"
            aws s3 rb s3://"$S3_BUCKET" --force --region "${{ secrets.AWS_REGION }}" || echo "Error eliminando S3"
          fi
          
          # Eliminar DynamoDB table
          if [ -n "$DYNAMODB_TABLE" ]; then
            echo "Eliminando DynamoDB table: $DYNAMODB_TABLE"
            aws dynamodb delete-table --table-name "$DYNAMODB_TABLE" --region "${{ secrets.AWS_REGION }}" || echo "Error eliminando DynamoDB"
          fi
          
          # Eliminar Secrets Manager secret
          if [ -n "$SECRET_ARN" ]; then
            echo "Eliminando Secrets Manager secret"
            aws secretsmanager delete-secret --secret-id "$SECRET_ARN" --force-delete-without-recovery --region "${{ secrets.AWS_REGION }}" || echo "Error eliminando Secret"
          fi
          
          # Eliminar IAM Role (que tiene las policies adjuntas)
          if [ -n "$IRSA_ROLE_ARN" ]; then
            ROLE_NAME=$(echo "$IRSA_ROLE_ARN" | cut -d'/' -f2)
            echo "Eliminando IAM Role: $ROLE_NAME"
            
            # Detach policies
            for policy_arn in $(aws iam list-attached-role-policies --role-name "$ROLE_NAME" --query 'AttachedPolicies[].PolicyArn' --output text 2>/dev/null); do
              aws iam detach-role-policy --role-name "$ROLE_NAME" --policy-arn "$policy_arn" || true
            done
            
            # Delete inline policies
            for policy_name in $(aws iam list-role-policies --role-name "$ROLE_NAME" --query 'PolicyNames[]' --output text 2>/dev/null); do
              aws iam delete-role-policy --role-name "$ROLE_NAME" --policy-name "$policy_name" || true
            done
            
            # Delete role
            aws iam delete-role --role-name "$ROLE_NAME" || echo "Error eliminando IAM Role"
          fi
          
          echo "Eliminación manual completada"

      # ---------------------------
      # 4) (Opcional) Borrar SOLO el state del microservicio
      #    NO toca el bucket compartido ni la tabla DynamoDB de locks
      # ---------------------------
      - name: Delete microservice terraform state files
        if: ${{ github.event.inputs.delete_backend == 'true' }}
        run: |
          set -e
          echo "Eliminando SOLO los archivos de state del microservicio"
          echo "   Bucket: $TF_BACKEND_BUCKET (NO será eliminado)"
          echo "   Keys a eliminar:"
          echo "     - $TF_BACKEND_KEY"
          echo "     - k8s/terraform/aws/terraform.tfstate"

          # Eliminar state files del microservicio
          aws s3api delete-object --bucket "$TF_BACKEND_BUCKET" --key "$TF_BACKEND_KEY" || true
          aws s3api delete-object --bucket "$TF_BACKEND_BUCKET" --key "k8s/terraform/aws/terraform.tfstate" || true
          
          echo "State files del microservicio eliminados"
          echo "Backend compartido (S3 + DynamoDB) conservado para otros servicios"

      - name: Verify cleanup
        if: steps.kubeconfig.outcome == 'success'
        continue-on-error: true
        run: |
          echo "Verificando limpieza de recursos..."
          
          # Verificar que no queden recursos en namespace documents
          if kubectl get namespace documents >/dev/null 2>&1; then
            echo "Namespace 'documents' todavía existe"
            kubectl get all -n documents || true
          else
            echo "Namespace 'documents' eliminado correctamente"
          fi
          
          # Verificar que no queden LoadBalancers del microservicio
          if kubectl get svc -A 2>/dev/null | grep -E "(documents|grafana)" >/dev/null; then
            echo "Algunos services todavía existen"
            kubectl get svc -A | grep -E "(documents|grafana)" || true
          else
            echo "Todos los services eliminados"
          fi

      - name: Done
        run: |
          echo "========================================"
          echo "Destroy del microservicio completado"
          echo "========================================"
          echo ""
          echo "Recursos eliminados:"
          echo "  S3 Bucket (documents)"
          echo "  DynamoDB Table (documents)"
          echo "  IAM Role/Policy (documents-irsa)"
          echo "  Secrets Manager (application config)"
          echo "  Monitoring stack (kube-prometheus)"
          echo "  LoadBalancers (Classic/NLB)"
          echo "  Namespace documents"
          echo "  Kubernetes resources (Deployments, Services, etc.)"
          echo ""
          echo "Recursos compartidos conservados:"
          echo "  VPC"
          echo "  EKS Cluster"
          echo "  RabbitMQ"
          echo "  AWS Load Balancer Controller"
          echo "  External Secrets Operator"
          echo ""
          echo "Para destruir la infraestructura compartida, ejecuta el workflow de destroy en el repo infrastructure-shared"
